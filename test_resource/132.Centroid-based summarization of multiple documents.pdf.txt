；Abstract
We present a multi-document summarizer, calledMEAD, which generates summaries usingcluster centroids produced by a topic detectionand tracking system. We also describe two newtechniques, based on sentence utility andsubsumption, which we have applied to theevaluation of both single and multiple documentsummaries. Finally, we describe two user studiesthat test our models of multi-documentsummarization.
；1 Introduction
On October 12, 1999, a relatively small number ofnews sources mentioned in passing that PakistaniDefense Minister Gen. Pervaiz Musharraf was awayvisiting Sri Lanka. However, all world agencieswould be actively reporting on the major events thatwere to happen in Pakistan in the following days:Prime Minister Nawaz Sharif announced that in Gen.Musharraf ’s absence, the Defense Minister had beensacked and replaced by General Zia Addin. Largenumbers of messages from various sources started toinundate the newswire: about the army’ s occupationof the capital, the Prime Minister ’ s ouster and hissubsequent placement under house arrest, Gen.Musharraf ’ s return to his country, his ascendancy topower, and the imposition of military control overPakistan.
The paragraph above summarizes a large amount ofnews from different sources. While it was notautomatically generated, one can imagine the use ofsuch automatically generated summaries. In thispaper we will describe how multi-documentsummaries are built and evaluated.
；；1.1 Topic detection and multi-documentsummarization
The process of identifying all articles on an emergingevent is called Topic Detection and Tracking (TDT).A large body of research in TDT has been createdover the past two years [Allan et al., 98]. We willpresent an extension of our own research on TDT[Radev et al., 1999] to cover summarization of multi-document clusters.
Our entry in the official TDT evaluation, calledCIDR [Radev et al., 1999], uses modified TF*IDF toproduce clusters of news articles on the same event.We developed a new technique for multi-documentsummarization (or MDS), called centroid-basedsummarization (CBS) which uses as input thecentroids of the clusters produced by CIDR toidentify which sentences are central to the topic ofthe cluster, rather than the individual articles. Wehave implemented CBS in a system, named MEAD.
The main contributions of this paper are: thedevelopment of a centroid-based multi-documentsummarizer, the use of cluster-based sentence utility(CBSU) and cross-sentence informationalsubsumption (CSIS) for evaluation of single andmulti-document summaries, two user studies thatsupport our findings, and an evaluation of MEAD.
An event cluster, produced by a TDT system,consists of chronologically ordered news articlesfrom multiple sources, which describe an event as itdevelops over time. Event clusters range from 2 to 10documents from which MEAD produces summariesin the form of sentence extracts.
A key feature of MEAD is its use of cluster centroids,which consist of words which are central not only toone article in a cluster, but to all the articles.
MEAD is significantly different from previous workon multi-document summarization [Radev &McKeown, 1998; Carbonell and Goldstein, 1998;Mani and Bloedorn, 1999; McKeown et al., 1999],
which use techniques such as graph matching,maximal marginal relevance, or language generation.
Finally, evaluation of multi-document summaries is adifficult problem. There is not yet a widely acceptedevaluation scheme. We propose a utility-basedevaluation scheme, which can be used to evaluateboth single-document and multi-documentsummaries.
；2 Informational content of sentences
；；2.1 Cluster-based sentence utility (CBSU)
Cluster-based sentence utility (CBSU, or utility)refers to the degree of relevance (from 0 to 10) of aparticular sentence to the general topic of the entirecluster (for a discussion of what is a topic, see [Allanet al. 1998]). A utility of 0 means that the sentence isnot relevant to the cluster and a 10 marks an essentialsentence.
；；2.2 Cross-sentence informationalsubsumption (CSIS)
A related notion to CBSU is cross-sentenceinformational subsumption (CSIS, or subsumption),which reflects that certain sentences repeat some ofthe information present in other sentences and may,therefore, be omitted during summarization. If theinformation content of sentence a (denoted as i(a)) iscontained within sentence b, then a becomesinformationally redundant and the content of b is saidto subsume that of a:
i(b)
i(a) ⊂In the example below, (2) subsumes (1) because thecrucial information in (1) is also included in (2)which presents additional content: “ the court” , “ lastAugust” , and “ sentenced him to life ” .
(1) John Doe was found guilty of the murder.(2) The court found John Doe guilty of the murderof Jane Doe last August and sentenced him to life.
The cluster shown in Figure 1 shows subsumptionlinks across two articles1 about recent terroristactivities in Algeria (ALG 18853 and ALG 18854).
An arrow from sentence A to sentence B indicatesthat the information content of A is subsumed by theinformation content of B. Sentences 2, 4, and 5 fromthe first article repeat the information from sentence
1  The full text of these articles is shown in theAppendix.
2 in the second article, while sentence 9 from theformer article is later repeated in sentences 3 and 4 ofthe latter article.
；；2.3 Equivalence classes of sentences
Sentences subsuming each other are said to belong tothe same equivalence class. An equivalence classmay contain more than two sentences within thesame or different articles. In the following example,although sentences (3) and (4) are not exactparaphrases of each other, they can be substituted foreach other without crucial loss of information andtherefore belong to the same equivalence class, i.e.i(3) ⊂ i(4) and i(4) ⊂ i(3). In the user study sectionwe will take a look at the way humans perceive CSISand equivalence class.
(3) Eighteen decapitated bodies have been foundin a mass grave in northern Algeria, press reportssaid Thursday.(4) Algerian newspapers have reported onThursday that 18 decapitated bodies have beenfound by the authorities.
；；2.4 Comparison with MMR
Maximal marginal relevance (or MMR) is atechnique similar to CSIS and was introduced in[Carbonell and Goldstein, 1998]. In that paper, MMRis used to produce summaries of single documentsthat avoid redundancy. The authors mention that theirpreliminary results indicate that multiple documentson the same topic also contain redundancy but theyfall short of using MMR for multi-documentsummarization. Their metric is used as anenhancement to a query-based summary whereasCSIS is designed for query-independent (a.k.a.,generic) summaries.
；3 MEAD: a centroid-based multi-document summarizer
We now describe the corpus used for the evaluationof MEAD, and later in this section we presentMEAD ’ s algorithm.
topicAlgerian terrorists threaten BelgiumThe FBI puts Osama bin Laden onthe most wanted listExplosion in a Moscow apartmentbuilding (September 9, 1999)Explosion in a Moscow apartmentbuilding (September 13, 1999)General strike in DenmarkToxic spill in Spain
；；3.1 Description of the corpus
For our experiments, we prepared a small corpusconsisting of a total of 558 sentences in 27documents, organized in 6 clusters (Table 1), allextracted by CIDR. Four of the clusters are fromUsenet newsgroups. The remaining two clusters arefrom the official TDT corpus2. Among the factors forour selection of clusters are: coverage of as manynews sources as possible, coverage of both TDT andnon-TDT data, coverage of different types of news(e.g., terrorism, internal affairs, and environment),and diversity in cluster sizes (in our case, from 2 to10 articles). The test corpus is used in the evaluationin such a way that each cluster is summarized at 9different compression rates, thus giving nine times asmany sample points as one would expect from thesize of the corpus.
；；3.2 Cluster centroids
Table 2 shows a sample centroid, produced by CIDR[Radev et al., 1999] from cluster A. The “ count”column indicates the average number of occurrencesof a word across the entire cluster. The IDF valueswere computed from the TDT corpus. A centroid, inthis context, is a pseudo-document which consists ofwords which have Count*IDF scores above a pre-defined threshold in the documents that constitute thecluster. CIDR computes Count*IDF in an iterativefashion, updating its values as more articles areinserted in a given cluster. We hypothesize thatsentences that contain the words from the centroidare more indicative of the topic of the cluster.
；；3.3 Centroid-based algorithm
MEAD decides which sentences to include in theextract by ranking them according to a set ofparameters. The input to MEAD is a cluster ofarticles (e.g., extracted by CIDR) and a value for thecompression rate r. For example, if the clustercontains a total of 50 sentences (n = 50) and thevalue of r is 20%, the output of MEAD will contain10 sentences. Sentences are laid in the same order asthey appear in the original documents withdocuments ordered chronologically. We benefit herefrom the time stamps associated with each document.
SCORE (s) = Σ i (wcCi + wpPi + wfFi)where i (1 ≤ i ≤ n) is the sentence number withinthe cluster.
INPUT: Cluster of d documents3 with n sentences(compression rate = r)
2 The selection of Cluster E is due to an idea by theparticipants in the Novelty Detection Workshop, ledby James Allan.
3 Note that currently, MEAD requires that sentenceboundaries be marked.
OUTPUT: (n * r) sentences from the clusterwith the highest values of SCORE.
The current paper evaluates two algorithms(pure centroid: wc = 1, wp = 0, wf  = 0) and(lead+centroid: wc = 1, wp = 1, wf  = 0).
；；3.4 Redundancy-based algorithm
We try to approximate CSIS by identifying sentencesimilarity across sentences. Its effect on MEAD is thesubtraction of a redundancy penalty (Rs) for eachsentence which overlaps with sentences that havehigher SCORE values. The redundancy penalty issimilar to the negative factor in the MMR formula[Carbonell and Goldstein, 1998].
SCORE (s) = Σ i (wcCi + wpPi + wfFi) - wRRs
For each pair of sentences extracted by MEAD, wecompute the cross-sentence word overlap accordingto the following formula:
Rs = 2 * (# overlapping words) / (# words in sentence1 + # words in sentence 2)
wR = Maxs (SCORE(s))Rs = 1 when the sentences are identical and Rs = 0when they have no words in common. Afterdeducting Rs, we rerank all sentences and possiblycreate a new sentence extract. We repeat this processuntil reranking doesn ’ t result in a different extract.
The number of overlapping words in the formula iscomputed in such a way that if a word appears mtimes in one sentence and n times in another, onlymin (m, n) of these occurrences will be consideredoverlapping.
；4 Techniques for evaluating summaries
Summarization evaluation methods can be dividedinto 2 categories: intrinsic and extrinsic [Mani andMaybury, 1999]. Intrinsic evaluation measures thequality of summaries directly (e.g., by comparingthem to ideal summaries). Extrinsic methods measurehow well the summaries help in performing aparticular task (e.g., classification). The extrinsicevaluation, also called task-based evaluation, hasreceived more attention recently at the DARPASummarization Evaluation Conference [Mani et al.,1998].
；；4.1 Single-document summaries
Two techniques commonly used to measureinterjudge agreement and to evaluate extracts are (A),precision and recall, and (B), percent agreement. In
both cases, an automatically generated summary iscompared against an “ ideal” summary. To constructthe ideal summary, a group of human subjects areasked to extract sentences. Then, the sentenceschosen by a majority of humans are included in theideal summary. The precision and recall indicate theoverlap between the ideal summary and the automaticsummary.
We should note that [Jing et al., 1998] pointed outthat the cut-off summary length can affect resultssignificantly, and the assumption of a single “ ideal''summary is problematic.
We will illustrate why these two methods are notsatisfactory. Suppose we want to determine which oftwo systems which selected summary sentences at acompression rate of  20% (Table 3) is better.
Using precision and recall indicates that theperformance of System 1 and System 2 is 50% and0%, respectively. System 2 appears to perform in theworst possible way since it is not possible todifferentiate between sentences S3 – S10, which areequally bad in this model. Using percent agreement,the performance is 80% and 60%, respectively,however percent agreement is highly dependent onthe compression rate.
；；4.2 Utility-based evaluation of both single andmultiple document summaries.
Instead of P&R or percent agreement, one canmeasure the coverage of the ideal summary’ s utility.In the example in Table 4, using both evaluationmethods A and B, System 1 achieves 50%, whereasSystem 2 achieves 0%. If we look at sentence utility,System 1 matches 18 out of 19 utility points in theideal summary and System 2 gets 15 out of 19. In thiscase, the performance of system 2 is not as low aswhen using methods A and B.
We therefore propose to model both interjudgeagreement and system evaluation as real-valuedvector matching and not as boolean (methods A andB). By giving credit for “ less than ideal'' sentencesand distinguishing the degree of importance betweensentences, the utility-based scheme is a more naturalmodel to evaluate summaries.
Other researchers have also suggested improvementson the precision and recall measure forsummarization. [Jing et al., 1998] proposed to usefractional P&R.  [Goldstein et al., 1999] used 11-point average precision.
；；；4.2.1 Interjudge agreement (J)
Without loss of generality, suppose that three judgesare asked to build extracts of a single article4. As anexample, Table 5 shows the weights of the differentsentences (note that no compression rate needs to bespecified; from the data in the table, one can generatesummaries at arbitrary compression rates).
The interjudge agreement measures, to what extenteach judge satisfies the utility of the other judges bypicking the right sentences.
In the example, with a 50% summary, Judge 1 wouldpick sentences 1 and 2 because they have themaximum utility as far as he is concerned. Judge 2would select the same two sentences, while Judge 3would pick 2 and 45. The maximum utilities for eachjudge are as follows: 18 (= 10 + 8), 19, and 17.
How well Judge 1 ’ s utility assignment satisfies Judge2’ s utility need? Since they have both selected the same sentences, Judge 1 achieves 19/19 (1.00) ofJudge 2 ’ s utility. However, Judge 1 only achieves13/17 (0.765) of Judge 3’ s utility.
We can therefore represent the cross-judge utilityagreement Ji,j as an asymmetric matrix (e.g., the valueof J1,2 is 0.765 while the value of J2,1 is 13/18 or0.722). The values Ji,j of the cross-judge utility matrixfor r = 50% are shown in Table 6.
We can also compute the performance of each judge(Ji) against all other judges by averaging for eachJudge i all values in the matrix Ji,j where i ≠ j. Thesenumbers indicate that Judge 3 is the outlier.
Finally, the mean cross-judge agreement J is theaverage of Ji for i=1..3. In the example, J = 0.841.J is like an upper bound on the performance of asummarizer (it can achieve a score higher than J onlywhen it can do a better job than the judges).
；；；4.2.2 Random performance (R)
We can also similarly define a lower bound on thesummarizer performance.
The random performance R is the average of allpossible system outputs at a given compression rate,r. For example, with 4 sentences and a r = 50%, theset of all possible system outputs is {12,13,14,23,24,34}6. For each of them, we can compute a systemperformance. For example, the system that selectssentences 1 and 4 (we label this system as {14})performs at 15/18 (or 0.833) against Judge 1, at 16/19against Judge 2 (or 0.842), and at 14/17 against Judge3 (or 0.824). On average, the performance of {14} isthe average of the three numbers, or 0.833.
We can compute the performance of all possiblesystems. The six numbers (in the order{12,13,14,23,24,34}) are 0.922, 0.627, 0.833, 0.631,0.837, and 0.543. Their average becomes the randomperformance (R) of all possible systems; in thisexample, R = 0.732.
；；；4.2.3 System performance (S)
The system performance S is one of the numbers6described in the previous subsection. For {13}, thevalue of S is 0.627 (which is lower than random). For{14}, S is 0.833, which is between R and J. In theexample, only two of the six possible sentenceselections, {14} and {24} are between R and J. Threeothers, {13}, {23}, and {34} are below R. while {12}is better than J.
；；；4.2.4. Normalized system performance (D)
To restrict system performance (mostly) between 0and 1, we use a mapping between R and J in such away that when S = R, the normalized systemperformance, D, is equal to 0 and when S = J, Dbecomes 1. The corresponding linear function7 is:
D = (S-R) / (J-R)
Figure 2 shows the mapping between systemperformance S on the left (a) and normalized systemperformance D on the right (b). A small part of the 0-1 segment is mapped to the entire 0-1 segment;therefore the difference between two systems,performing at e.g., 0.785 and 0.812 can besignificant!
0.732) or 0.963. Of the two systems, {24}outperforms {14}.
；；4.3 Using CSIS to evaluate multi-documentsummaries
To use CSIS in the evaluation, we introduce a newparameter, E, which tells us how much to penalize asystem that includes redundant information. In theexample from Table 7 (arrows indicate subsumption),a summarizer with r = 20% needs to pick 2 out of 12sentences. Suppose that it picks 1/1 and 2/1 (in bold).If E = 1, it should get full credit of 20 utility points. IfE = 0, it should get no credit for the second sentenceas it is subsumed by the first sentence. By varying Ebetween 0 and 1, the evaluation may favor or ignoresubsumption.
We ran two user experiments. First, six judges wereeach given six clusters and asked to ascribe animportance score from 0 to 10 to each sentencewithin a particular cluster. Next, five judges had toindicate for each sentence which other sentence(s), ifany, it subsumes8.
；；5.1 CBSU: interjudge agreement
Using the techniques described in Section 0, wecomputed the cross-judge agreement (J) for the 6clusters for various r (Figure 3). Overall, interjudgeagreement was quite high. An interesting drop ininterjudge agreement occurs for 20-30% summaries.The drop most likely results from the fact that 10%summaries are typically easier to produce because thefew most important sentences in a cluster are easierto identify.
Example: the normalized system performance for the{14} system then becomes (0.833 - 0.732) / (0.841 –0.732) or 0.927. Since the score is close to 1, the{14} system is almost as good as the interjudgeagreement. The normalized system performance forthe {24} system is similarly (0.837 – 0.732) / (0.841
7 The formula is valid when J > R (that is, the judgesagree among each other better than randomly).
8 We should note that both annotation tasks werequite time consuming and frustrating for the userswho took anywhere from 6 to 10 hours each tocomplete their part.
；；5.2 CSIS: interjudge agreement
In the second experiment, we asked users to indicateall cases when within a cluster, a sentence issubsumed by another. The judges ’ data on the firstseven sentences of cluster A are shown in Table 8.
The “ + score ” indicates the number of judges whoagree on the most frequent subsumption. The “ -score ” indicates that the consensus was nosubsumption. We found relatively low interjudgeagreement on the cases in which at least one judgeindicated evidence of subsumption. Overall, out of558 sentences, there was full agreement (5 judges) on292 sentences (Table 9). Unfortunately, in 291 ofthese 292 sentences the agreement was that there isno subsumption. When the bar of agreement waslowered to four judges, 23 out of 406 agreements areon sentences with subsumption. Overall, out of 80sentences with subsumption, only 24 had anagreement of four or more judges. However, in 54cases at least three judges agreed on the presence of aparticular instance of subsumption.
In conclusion, we found very high interjudgeagreement in the first experiment and moderatelylow agreement in the second experiment. Weconcede that the time necessary to do a proper jobat the second task is partly to blame.
；；5.3 Evaluation of MEAD
Since the baseline of random sentence selection isalready included in the evaluation formulae, weused the Lead-based method (selecting the
positionally first (n*r/c) sentences from each clusterwhere c = number of clusters) as the baseline toevaluate our system.
In Table 10 we show the normalized performance(D) of MEAD, for the six clusters at ninecompression rates. MEAD performed better thanLead in 29 (in bold) out of 54 cases. Note that forthe largest cluster, Cluster D, MEAD outperformedLead at all compression rates.
We then modified the MEAD algorithm to includelead information as well as centroids (see Section 0).In this case, MEAD+Lead performed better than theLead baseline in 41 cases. We are in the process ofrunning experiments with other SCORE formulas.
；；5.4 Discussion
It may seem that utility-based evaluation requires toomuch effort and is prone to low interjudge agreement.We believe that our results show that interjudgeagreement is quite high. As far as the amount ofeffort required, we believe that the larger effort onthe part of the judges is more or less compensatedwith the ability to evaluate summaries off-line and atvariable compression rates. Alternative evaluationsdon ’ t make such evaluations possible. We shouldconcede that a utility-based approach is probably notfeasible for query-based summaries as these aretypically done only on-line.
We discussed the possibility of a sentencecontributing negatively to the utility of anothersentence due to redundancy. We should also point outthat sentences can also reinforce one anotherpositively. For example, if a sentence mentioning anew entity is included in a summary, one might alsowant to include a sentence that puts the entity in thecontext of the rest of the article or cluster.
；6 Contributions and future work
We presented a new multi-document summarizer,MEAD. It summarizes clusters of news articlesautomatically grouped by a topic detection system.MEAD uses information from the centroids of theclusters to select sentences that are most likely to berelevant to the cluster topic.
We used a new utility-based technique, CBSU, forthe evaluation of MEAD and of summarizers ingeneral. We found that MEAD produces summariesthat are similar in quality to the ones produced byhumans. We also compared MEAD ’ s performance toan alternative method, multi-document lead, and
showed how MEAD ’ s sentence scoring weights canbe modified to produce summaries significantlybetter than the alternatives.
We also looked at a property of multi-documentclusters, namely cross-sentence informationsubsumption (which is related to the MMR metricproposed in [Carbonell and Goldstein, 1998]) andshowed how it can be used in evaluating multi-document summaries.
All our findings are backed by the analysis of twoexperiments that we performed with human subjects.We found that the interjudge agreement on sentenceutility is very high while the agreement on cross-sentence subsumption is moderately low, althoughpromising.
In the future, we would like to test ourmultidocument summarizer on a larger corpus andimprove the summarization algorithm. We wouldalso like to explore how the techniques we proposedhere can be used for multiligual multidocumentsummarization.
；7 Acknowledgments
We would like to thank Inderjeet Mani, WlodekZadrozny, Rie Kubota Ando, Joyce Chai, and NandaKambhatla for their valuable feedback. We wouldalso like to thank Carl Sable, Min-Yen Kan, DaveEvans, Adam Budzikowski, and Veronika Horvathfor their help with the evaluation.
；References
James Allan, Jaime Carbonell, George Doddington,Jonathan Yamron, and Yiming Yang, Topicdetection and tracking pilot study: final report, InProceedings of the Broadcast News Understandingand Transcription Workshop, 1998.Jaime Carbonell and Jade Goldstein. The use ofMMR, diversity-based reranking for reordering
documents and producing summaries. InProceedings of ACM-SIGIR’ 98, Melbourne,Australia, August 1998.Jade Goldstein, Mark Kantrowitz, Vibhu Mittal, andJaime Carbonell, Summarizing Text Documents:Sentence Selection and Evaluation Metrics, InProceedings of ACM-SIGIR’ 99, Berkeley, CA,August 1999.Thérèse Hand. A Proposal for Task-Based Evaluationof Text Summarization Systems, in Mani, I., andMaybury, M., eds., Proceedings of theACL/EACL'97 Workshop on Intelligent ScalableText Summarization, Madrid, Spain, July 1997.Hongyan Jing, Regina Barzilay, Kathleen McKeown,and Michael Elhadad, Summarization EvaluationMethods: Experiments and Analysis, In WorkingNotes, AAAI Spring Symposium on IntelligentText Summarization, Stanford, CA, April 1998.Inderjeet Mani and Eric Bloedorn, SummarizingSimilarities and Differences Among RelatedDocuments, Information Retrieval 1 (1-2), pages35-67, June 1999.
Inderjeet Mani, David House, Gary Klein, LynetteHirschman, Leo Orbst, Thérèse Firmin, MichaelChrzanowski, and Beth Sundheim. The TIPSTERSUMMAC text summarization evaluation.Technical Report MTR98W0000138, MITRE,McLean, Virginia, October 1998.Inderjeet Mani and Mark Maybury. Advances inAutomatic Text Summarization. MIT Press, 1999.Kathleen McKeown, Judith Klavans, VasileiosHatzivassiloglou, Regina Barzilay, and EleazarEskin, Towards Multidocument Summarization byReformulation: Progress and Prospects, InProceedings of AAAI ’ 99, Orlando, FL, July 1999.Dragomir R. Radev and Kathleen McKeown.Generating natural language summaries frommultiple on-line sources. ComputationalLinguistics, 24 (3), pages 469-500, September1998.Dragomir R. Radev, Vasileios Hatzivassiloglou, andKathleen R. McKeown. A description of the CIDRsystem as used for TDT-2. In DARPA BroadcastNews Workshop, Herndon, VA, February 1999.
；Appendix
ARTICLE 18853: ALGIERS, May 20 (AFP)
ARTICLE 18854: ALGIERS, May 20 (UPI)
1. Eighteen decapitated bodies have been found in amass grave in northern Algeria, press reports saidThursday, adding that two shepherds were murderedearlier this week.
2. Security forces found the mass grave on Wednesdayat Chbika, near Djelfa, 275 kilometers (170 miles)south of the capital.
3. It contained the bodies of people killed last yearduring a wedding ceremony, according to Le QuotidienLiberte.
4. The victims included women, children and old men.
5. Most of them had been decapitated and their headsthrown on a road, reported the Es Sahafa.
6. Another mass grave containing the bodies of around10 people was discovered recently near Algiers, in theEucalyptus district.
7. The two shepherds were killed Monday evening by agroup of nine armed Islamists near the Moulay Slissenforest.
8. After being injured in a hail of automatic weaponsfire, the pair were finished off with machete blowsbefore being decapitated, Le Quotidien d’ Oran reported.
9. Seven people, six of them children, were killed andtwo injured Wednesday by armed Islamists nearMedea, 120 kilometers (75 miles) south of Algiers,security forces said.
10. The same day a parcel bomb explosion injured 17people in Algiers itself.
11. Since early March, violence linked to armedIslamists has claimed more than 500 lives, according topress tallies.
1. Algerian newspapers have reported that 18decapitated bodies have been found by authoritiesin the south of the country.
2. Police found the “ decapitated bodies of women,children and old men,with their heads thrown on aroad'' near the town of Jelfa, 275 kilometers (170miles) south of the capital Algiers.
3. In another incident on Wednesday, seven people-- including six children -- were killed by terrorists,Algerian security forces said.
4. Extremist Muslim militants were responsible forthe slaughter of the seven people in the province ofMedea, 120 kilometers (74 miles) south of Algiers.
5. The killers also kidnapped three girls during thesame attack, authorities said, and one of the girlswas found wounded on a nearby road.
6. Meanwhile, the Algerian daily Le Matin todayquoted Interior Minister  Abdul Malik Silal assaying that “ terrorism has not been eradicated, butthe movement of the terrorists has significantlydeclined.''
7. Algerian violence has claimed the lives of morethan 70,000 people since the army cancelled the1992 general elections that Islamic parties werelikely to win.
8. Mainstream Islamic groups, most of which arebanned in the country, insist their members are notresponsible for the violence against civilians.
9. Some Muslim groups have blamed the army,while others accuse “ foreign elements conspiringagainst Algeria.’